LD=5
Using: cuda
Train proportions: tensor([0.2509, 0.2442, 0.2506, 0.2543])
Test proportions: tensor([0.2463, 0.2732, 0.2476, 0.2329])
Epoch 1/500: train_loss: 0.9672 val_loss 0.7113
Saved at epoch 1.
Epoch 2/500: train_loss: 0.6472 val_loss 0.5897
Saved at epoch 2.
Epoch 3/500: train_loss: 0.5781 val_loss 0.5282
Saved at epoch 3.
Epoch 4/500: train_loss: 0.5329 val_loss 0.4900
Saved at epoch 4.
Epoch 5/500: train_loss: 0.5042 val_loss 0.4652
Saved at epoch 5.
Epoch 6/500: train_loss: 0.3529 val_loss 0.2052
Saved at epoch 6.
Epoch 7/500: train_loss: 0.1828 val_loss 0.1518
Saved at epoch 7.
Epoch 8/500: train_loss: 0.1531 val_loss 0.1340
Saved at epoch 8.
Epoch 9/500: train_loss: 0.1349 val_loss 0.1171
Saved at epoch 9.
Epoch 10/500: train_loss: 0.1187 val_loss 0.1073
Saved at epoch 10.
Epoch 11/500: train_loss: 0.1056 val_loss 0.1017
Saved at epoch 11.
Epoch 12/500: train_loss: 0.0950 val_loss 0.1017
Saved at epoch 12.
Epoch 13/500: train_loss: 0.0885 val_loss 0.0882
Saved at epoch 13.
Epoch 14/500: train_loss: 0.0807 val_loss 0.0835
Saved at epoch 14.
Epoch 15/500: train_loss: 0.0758 val_loss 0.0824
Saved at epoch 15.
Epoch 16/500: train_loss: 0.0709 val_loss 0.0771
Saved at epoch 16.
Epoch 17/500: train_loss: 0.0674 val_loss 0.0760
Saved at epoch 17.
Epoch 18/500: train_loss: 0.0641 val_loss 0.0807
Epoch 19/500: train_loss: 0.0630 val_loss 0.0815
Epoch 20/500: train_loss: 0.0600 val_loss 0.0749
Saved at epoch 20.
Epoch 21/500: train_loss: 0.0560 val_loss 0.0755
Epoch 22/500: train_loss: 0.0542 val_loss 0.0838
Epoch 23/500: train_loss: 0.0529 val_loss 0.0740
Saved at epoch 23.
Epoch 24/500: train_loss: 0.0514 val_loss 0.0749
Epoch 25/500: train_loss: 0.0489 val_loss 0.0778
Epoch 26/500: train_loss: 0.0474 val_loss 0.0747
Epoch 27/500: train_loss: 0.0461 val_loss 0.0750
Epoch 28/500: train_loss: 0.0450 val_loss 0.0740
Epoch 29/500: train_loss: 0.0438 val_loss 0.0764
Epoch 30/500: train_loss: 0.0424 val_loss 0.0755
Epoch 31/500: train_loss: 0.0413 val_loss 0.0746
Epoch 32/500: train_loss: 0.0405 val_loss 0.0757
Epoch 33/500: train_loss: 0.0402 val_loss 0.0907
Epoch 34/500: train_loss: 0.0396 val_loss 0.0743
Epoch 35/500: train_loss: 0.0382 val_loss 0.0771
Epoch 36/500: train_loss: 0.0370 val_loss 0.0782
Epoch 37/500: train_loss: 0.0371 val_loss 0.0758
Epoch 38/500: train_loss: 0.0360 val_loss 0.0859
Epoch 39/500: train_loss: 0.0350 val_loss 0.0766
Epoch 40/500: train_loss: 0.0345 val_loss 0.0786
Epoch 41/500: train_loss: 0.0343 val_loss 0.0786
Epoch 42/500: train_loss: 0.0328 val_loss 0.0795
Epoch 43/500: train_loss: 0.0318 val_loss 0.0802
Epoch 44/500: train_loss: 0.0316 val_loss 0.0804
Epoch 45/500: train_loss: 0.0311 val_loss 0.0815
Epoch 46/500: train_loss: 0.0312 val_loss 0.0808
Epoch 47/500: train_loss: 0.0301 val_loss 0.0823
Epoch 48/500: train_loss: 0.0302 val_loss 0.0822
Epoch 49/500: train_loss: 0.0294 val_loss 0.0808
Epoch 50/500: train_loss: 0.0295 val_loss 0.0839
Epoch 51/500: train_loss: 0.0282 val_loss 0.0826
Epoch 52/500: train_loss: 0.0283 val_loss 0.0833
Epoch 53/500: train_loss: 0.0278 val_loss 0.0847
Epoch 54/500: train_loss: 0.0283 val_loss 0.0836
Epoch 55/500: train_loss: 0.0277 val_loss 0.0897
Epoch 56/500: train_loss: 0.0268 val_loss 0.0831
Epoch 57/500: train_loss: 0.0268 val_loss 0.0864
Epoch 58/500: train_loss: 0.0260 val_loss 0.0831
Epoch 59/500: train_loss: 0.0265 val_loss 0.0834
Epoch 60/500: train_loss: 0.0269 val_loss 0.0882
Epoch 61/500: train_loss: 0.0259 val_loss 0.0906
Epoch 62/500: train_loss: 0.0255 val_loss 0.0895
Epoch 63/500: train_loss: 0.0268 val_loss 0.0861
Epoch 64/500: train_loss: 0.0263 val_loss 0.0872
Epoch 65/500: train_loss: 0.0255 val_loss 0.0857
Epoch 66/500: train_loss: 0.0257 val_loss 0.0860
Epoch 67/500: train_loss: 0.0245 val_loss 0.0881
Epoch 68/500: train_loss: 0.0250 val_loss 0.0879
Epoch 69/500: train_loss: 0.0256 val_loss 0.0865
Epoch 70/500: train_loss: 0.0241 val_loss 0.0896
Epoch 71/500: train_loss: 0.0230 val_loss 0.0889
Epoch 72/500: train_loss: 0.0249 val_loss 0.0880
Epoch 73/500: train_loss: 0.0229 val_loss 0.0885
Early stopping at epoch 73.
Test Accuracy: 98.05%
